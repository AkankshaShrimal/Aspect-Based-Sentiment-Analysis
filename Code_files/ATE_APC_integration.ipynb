{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ATE_APC_integration.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ZOvxcS4O5tle","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638095267753,"user_tz":-330,"elapsed":16804,"user":{"displayName":"Akanksha Shrimal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01375761960509741258"}},"outputId":"8495e6dc-7c56-4d26-95d7-fbfbf55619f3"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"m72r80Bc6vUJ"},"source":["Import"]},{"cell_type":"code","metadata":{"id":"j64u7gm94b9C","executionInfo":{"status":"ok","timestamp":1638095270237,"user_tz":-330,"elapsed":2490,"user":{"displayName":"Akanksha Shrimal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01375761960509741258"}}},"source":["import numpy as np\n","import pandas as pd\n","from keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","from keras.models import Model, Input\n","from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional,Conv1D,concatenate\n","from tensorflow import keras\n"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"fu_JCDKAbXM4","executionInfo":{"status":"ok","timestamp":1638095276109,"user_tz":-330,"elapsed":5875,"user":{"displayName":"Akanksha Shrimal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01375761960509741258"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import pandas as pd \n","import os\n","import pickle\n","import numpy as np\n","import xml.etree.ElementTree as ET\n","from torch.utils.data import Dataset\n","\n","import os\n","import torch\n","import torch.nn as nn\n","import argparse\n","from sklearn import metrics"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"yQeMOcUvbXTu","executionInfo":{"status":"ok","timestamp":1638095276109,"user_tz":-330,"elapsed":6,"user":{"displayName":"Akanksha Shrimal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01375761960509741258"}}},"source":["from torch.utils.data import DataLoader"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2R5Rp_Gz6xjc"},"source":["Load/Save"]},{"cell_type":"code","metadata":{"id":"GQwOpEgg4mPC","executionInfo":{"status":"ok","timestamp":1638095276110,"user_tz":-330,"elapsed":5,"user":{"displayName":"Akanksha Shrimal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01375761960509741258"}}},"source":["import pickle\n","import joblib\n","\n","#Load Save Pickle Utility\n","def save(filename, obj):\n","  with open(filename, 'wb') as handle:\n","      joblib.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","def load(filename):\n","  with open(filename, 'rb') as handle:\n","      return joblib.load(filename)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"35GUsHPJbkvy"},"source":["Required Model classes and code for ATC"]},{"cell_type":"markdown","metadata":{"id":"4lK5pwW5b5zD"},"source":["#### Models "]},{"cell_type":"code","metadata":{"id":"RuUD3R0MbozH","executionInfo":{"status":"ok","timestamp":1638095277048,"user_tz":-330,"elapsed":943,"user":{"displayName":"Akanksha Shrimal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01375761960509741258"}}},"source":["# Models \n","class Squeezer(nn.Module):\n","    \n","    #Squeeze sequence embedding length to the longest one in the batch\n","    \n","    def __init__(self, batch_first=True):\n","        super(Squeezer, self).__init__()\n","        self.batch_first = batch_first\n","    \n","    def forward(self, x, x_len):\n","        # sequence , sort , pad and pack , unpack , unsort\n","        \n","        # sorting the given x lengths values \n","        x_sort_idx = torch.sort(x_len, descending=True)[1].long()\n","        x_unsort_idx = torch.sort(x_sort_idx)[1].long()\n","        x_len = x_len[x_sort_idx]\n","        x_len = np.array(x_len.cpu())\n","        x = x[x_sort_idx]\n","        '''pack'''\n","        x_emb_p = torch.nn.utils.rnn.pack_padded_sequence(x, x_len, batch_first=self.batch_first)\n","        '''unpack'''\n","        out, _ = torch.nn.utils.rnn.pad_packed_sequence(x_emb_p, batch_first=self.batch_first)\n","        if self.batch_first:\n","            out = out[x_unsort_idx]\n","        else:\n","            out = out[:, x_unsort_idx]\n","        return out\n","\n","class Recurrent_Models(nn.Module):\n","    '''\n","    LSTM which can hold variable length sequence, use like TensorFlow's RNN(input, lenght...).\n","    '''\n","    def __init__(self, input_size, hidden_size, num_layers=1, bias=True, batch_first=True, dropout=0,\n","                 bidirectional=False, only_use_last_hidden_state=False, rnn_type='LSTM'):\n","        super(Recurrent_Models, self).__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.bias = bias\n","        self.batch_first = batch_first\n","        self.dropout = dropout\n","        self.bidirectional = bidirectional\n","        self.only_use_last_hidden_state = only_use_last_hidden_state\n","        self.rnn_type = rnn_type\n","        \n","        if self.rnn_type == 'LSTM':\n","            self.RNN = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers,\n","                               bias=bias, batch_first=batch_first, dropout=dropout, bidirectional=bidirectional)\n","        elif self.rnn_type == 'GRU':\n","            self.RNN = nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers,\n","                              bias=bias, batch_first=batch_first, dropout=dropout, bidirectional=bidirectional)\n","        elif self.rnn_type == 'RNN':\n","            self.RNN = nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers,\n","                              bias=bias, batch_first=batch_first, dropout=dropout, bidirectional=bidirectional)\n","    \n","    def forward(self, x, x_len):\n","        \n","        '''\n","        sequence -> sort -> pad and pack -> process using RNN -> unpack -> unsort\n","        '''\n","        '''sort'''\n","        x_sort_idx = torch.sort(x_len, descending=True)[1].long()\n","        x_unsort_idx = torch.sort(x_sort_idx)[1].long()\n","\n","        \n","        x_len = x_len[x_sort_idx]\n","        x_len = np.array(x_len.cpu())\n","        x = x[x_sort_idx]\n","        \n","        '''pack'''\n","        x_emb_p = torch.nn.utils.rnn.pack_padded_sequence(x, x_len, batch_first=self.batch_first)\n","        ''' process '''\n","        if self.rnn_type == 'LSTM':\n","            out_pack, (ht, ct) = self.RNN(x_emb_p, None)\n","        else:\n","            out_pack, ht = self.RNN(x_emb_p, None)\n","            ct = None\n","        '''unsort'''\n","        ht = ht[:, x_unsort_idx]\n","        if self.only_use_last_hidden_state:\n","            return ht\n","        else:\n","            out, _ = torch.nn.utils.rnn.pad_packed_sequence(out_pack, batch_first=self.batch_first)\n","            if self.batch_first:\n","                out = out[x_unsort_idx]\n","            else:\n","                out = out[:, x_unsort_idx]\n","            if self.rnn_type == 'LSTM':\n","                ct = ct[:, x_unsort_idx]\n","            return out, (ht, ct)\n","\n","# PBAN Model \n","class PBAN(nn.Module):\n","    # Position-aware bidirectional attention network \n","    # paper https://aclanthology.org/C18-1066.pdf\n","\n","    # if embedding matrix not one then glove or word to vec used else brt used for embeddings \n","    # hyper_param is a dictionary containing all required hyperparameters \n","    def __init__(self, embedding_matrix, hyper_param):\n","\n","        super(PBAN, self).__init__()\n","        # embeddings of the text \n","        \n","        self.embed = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float))\n","   \n","     \n","        # randomly intialised embedding for position with respect to aspect  \n","        self.pos_embed = nn.Embedding(hyper_param['max_length'], hyper_param['position_dim'])\n","        # Left GRU taking aspect \n","        self.aspect_gru = Recurrent_Models(hyper_param['embed_dim'], hyper_param['hidden_dim'], num_layers=1, \n","                                    batch_first=True, bidirectional=True, rnn_type='GRU')\n","        # Right GRU taking input as words and position embedding \n","        self.sentence_position_gru = Recurrent_Models(hyper_param['embed_dim']+hyper_param['position_dim'], hyper_param['hidden_dim'], num_layers=1, \n","                                     batch_first=True, bidirectional=True, rnn_type='GRU')\n","        \n","\n","        self.weight_m = nn.Parameter(torch.Tensor(hyper_param['hidden_dim']*2, hyper_param['hidden_dim']*2))\n","        self.bias_m = nn.Parameter(torch.Tensor(1))\n","        self.weight_n = nn.Parameter(torch.Tensor( hyper_param['hidden_dim']*2, hyper_param['hidden_dim']*2))\n","        self.bias_n = nn.Parameter(torch.Tensor(1))\n","        self.w_r = nn.Linear( hyper_param['hidden_dim']*2,  hyper_param['hidden_dim'])\n","        self.w_s = nn.Linear( hyper_param['hidden_dim'], hyper_param['polarities_dim'])\n","    \n","    def forward(self, X):\n","        # getting text , aspect term and position indices \n","        text, aspect_text, position_tag = X[0], X[1], X[2]\n","  \n","        x = self.embed(text)\n","          #Sentence representation from any glove or wrod to vec '''\n","          \n","        ''' Hidden Representation from Sentence and Position knowledge '''\n","        # getting position embedding \n","        position_embedding = self.pos_embed(position_tag)\n","        # getting lengths of all batch sentences into a array x_len (removing padded 0)\n","        x_len = torch.sum(text != 0, dim=-1)\n","        # passing sentence and position embedding to gru \n","        x = torch.cat((position_embedding, x), dim=-1)\n","        h_x, _ = self.sentence_position_gru(x, x_len)\n","\n","        ''' Aspect term representation '''\n","        aspect_embedding = self.embed(aspect_text)\n","        aspect_len = torch.sum(aspect_text != 0, dim=-1)\n","        h_t, _ = self.aspect_gru(aspect_embedding, aspect_len)\n","\n","        # Attention Calculation \n","        ''' Aspect term to position-aware sentence attention '''\n","         \n","        alpha = F.softmax(torch.tanh(torch.add(torch.bmm(torch.matmul(h_t, self.weight_m), torch.transpose(h_x, 1, 2)), self.bias_m)), dim=1)\n","        s_x = torch.bmm(alpha, h_x)\n","        ''' Position-aware sentence attention to aspect term '''\n","        h_x_pool = torch.unsqueeze(torch.div(torch.sum(h_x, dim=1), x_len.float().view(x_len.size(0), 1)), dim=1)\n","        gamma = F.softmax(torch.tanh(torch.add(torch.bmm(torch.matmul(h_x_pool, self.weight_n), torch.transpose(h_t, 1, 2)), self.bias_n)), dim=1)\n","        h_r = torch.squeeze(torch.bmm(gamma, s_x), dim=1)\n","        ''' Output transform '''\n","        out = torch.tanh(self.w_r(h_r))\n","        out = self.w_s(out)\n","        return out\n","\n","class LSTM(nn.Module):\n","    # standard LSTM output without any consideration of target  \n","    def __init__(self, embedding_matrix, hyper_param):\n","        super(LSTM, self).__init__()\n","        \n","        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float))\n","        self.lstm = Recurrent_Models(hyper_param['embed_dim'], hyper_param['hidden_dim'], num_layers=1, batch_first=True)\n","        self.out = nn.Linear(hyper_param['hidden_dim'], hyper_param['polarities_dim'])\n","    \n","    def forward(self, x):\n","        # extracting only text \n","        \n","        text = x[0]\n","        # substituting words by embedding\n","        \n","        x = self.embedding(text)\n","        x_len = torch.sum(text != 0, dim=-1)\n","        \n","        _, (hidden, _) = self.lstm(x, x_len)\n","        out = self.out(hidden[0])\n","        return out\n","\n","\n","class AE_LSTM(nn.Module):\n","    ''' LSTM with Aspect Embedding '''\n","    def __init__(self, embedding_matrix, hyper_param):\n","        super(AE_LSTM, self).__init__()\n","        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float))\n","        self.squeeze_embedding = Squeezer()\n","        self.lstm = Recurrent_Models(hyper_param['embed_dim']*2, hyper_param['hidden_dim'], num_layers=1, batch_first=True)\n","        self.dense_layer = nn.Linear(hyper_param['hidden_dim'], hyper_param['polarities_dim'])\n","    \n","    def forward(self, x):\n","        # extracting text and aspect words from given batch of sentences\n","        text_words, aspect_text = x[0], x[1]\n","        \n","        # finding max length among all aspects and text  \n","        x_len = torch.sum(text_words != 0, dim=-1)\n","        x_len_max = torch.max(x_len)\n","        aspect_len = torch.sum(aspect_text != 0, dim=-1).float()\n","        \n","        # Geting embedding of aspects and words or texts \n","        x = self.embedding(text_words)\n","        x = self.squeeze_embedding(x, x_len)\n","        aspect = self.embedding(aspect_text)\n","        # concatenating aspect and all the words respectively \n","        aspect_pool = torch.div(torch.sum(aspect, dim=1), aspect_len.view(aspect_len.size(0), 1))\n","        aspect = torch.unsqueeze(aspect_pool, dim=1).expand(-1, x_len_max, -1)\n","        x = torch.cat((aspect, x), dim=-1)\n","        # sending input to lstm \n","        _, (hidden, _) = self.lstm(x, x_len)\n","        out = self.dense_layer(hidden[0])\n","        return out"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IUBUaQ-1b8YH"},"source":["#### Preprocessing and evaluate function"]},{"cell_type":"code","metadata":{"id":"VMlodxflb9-g","executionInfo":{"status":"ok","timestamp":1638095277049,"user_tz":-330,"elapsed":10,"user":{"displayName":"Akanksha Shrimal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01375761960509741258"}}},"source":["def evaluate(model,test_dataloader):\n","    # switch model to evaluation mode\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model.eval()\n","    test_correct, test_total = 0, 0\n","    y_complete, y_hat_complete = None, None\n","    \n","    with torch.no_grad():\n","        for idx,batch in enumerate(test_dataloader):\n","            \n","            x = [batch[col].to(device) for col in ['text', 'aspect', 'position']]\n","            y = batch['polarity'].to(device)\n","            outputs = model(x)\n","            \n","            test_correct += (torch.argmax(outputs, -1) == y).sum().item()\n","            test_total += len(outputs)\n","            # appending all y and y hats  \n","            if y_complete is not None:\n","              y_complete = torch.cat((y_complete, y), dim=0)\n","            else:\n","              y_complete = y\n","            if y_hat_complete is not None:\n","              y_hat_complete = torch.cat((y_hat_complete, outputs), dim=0)\n","            else:\n","              y_hat_complete = outputs\n","\n","    test_acc = test_correct / test_total\n","    f1 = metrics.f1_score(y_complete.cpu(), torch.argmax(y_hat_complete, -1).cpu(), labels=[0, 1, 2], average='macro')\n","    return test_acc, f1\n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"86J9-BJVC5un","executionInfo":{"status":"ok","timestamp":1638095277049,"user_tz":-330,"elapsed":9,"user":{"displayName":"Akanksha Shrimal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01375761960509741258"}}},"source":["def create_vocab(arr):\n","  vocab = dict()\n","  idx = 0 \n","  # padding represented by 0 and on index 0 \n","  vocab[0] = 0\n","  idx = idx+1\n","  vocab['unk'] = 1\n","  \n","  for sen in arr:\n","    for w in sen:\n","      if w not in vocab.keys() and  w != 0:\n","        idx = idx+1\n","        vocab[w] = idx\n","  inv_vocab = {v: k for k, v in vocab.items()} \n","  return vocab , inv_vocab\n","\n","def word_to_id(vocab,w):\n","  if w not in vocab.keys():\n","    return 1\n","  else:\n","    return vocab[w]\n","\n","\n","def id_to_word(inv_vocab,id):\n","  return inv_vocab[id]\n"," \n"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"TWnoMjAfbo7P","executionInfo":{"status":"ok","timestamp":1638095277050,"user_tz":-330,"elapsed":9,"user":{"displayName":"Akanksha Shrimal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01375761960509741258"}}},"source":["def getResults(data):\n","  rows = []\n","  # Iterate through every row\n","  for i in range(len(data.index)):\n","      # Collect all occurences of B\n","      indices_B = [k for k, x in enumerate(data.loc[i][\"aspect_labels\"]) if x == 'B']\n","\n","      tempListA = data.loc[i][\"terms\"]\n","\n","      # Iterate through every index\n","      for j in indices_B:\n","        tempListB = ['O'] * len(data.loc[i][0])\n","        tempListB[j] = \"B\"\n","\n","        for k in range(j + 1, len(data.loc[i][0])):\n","          if data.loc[i][\"aspect_labels\"][k] == \"I\":\n","            if tempListB[k-1] == \"B\" or tempListB[k-1] == \"I\":\n","              tempListB[k] = \"I\"\n","\n","        tempListC = ['NA'] * len(data.loc[i][0])\n","        senti = data.loc[i][\"sentiments\"][j]\n","      \n","      \n","        rows.append([tempListA, tempListB, senti])\n","\n","  result = pd.DataFrame(rows, columns = [\"terms\", \"aspect_labels\", \"sentiments\"])\n","\n","  return result\n","\n","def prepare_dataset(vocab, df, max_length):\n","\n","  number = df.shape[0]\n","  # returns all text element in data \n","  text_data = np.zeros([number,max_length])\n","  aspect_data = np.zeros([number,max_length])\n","  position_data = np.zeros([number,max_length])\n","  sentiment = np.zeros([number])\n","\n","  # flag to keep a track of how many aspect terms inserted \n","  \n","  for idx, sen in enumerate(df['terms'].to_numpy()):\n","    flag = 0\n","    for wid , w in enumerate(sen):\n","      # text number pusshed \n","      text_data[idx][wid] = word_to_id(vocab,w)\n","\n","      # aspect number pushed \n","      if df['aspect_labels'].iloc[idx][wid] == 'B' or df['aspect_labels'].iloc[idx][wid] == 'I':\n","        \n","        aspect_data[idx][flag] = word_to_id(vocab,w)\n","        flag = flag + 1\n","        \n","    # sentiment of sentence added \n","    if df['sentiments'][idx] == 'NEU':\n","      sentiment[idx] = 2\n","    elif df['sentiments'][idx]  == 'POS':\n","      sentiment[idx] = 0 \n","    else:\n","      sentiment[idx] = 1\n","\n","    # position embedding \n","    x1 = np.array(df['aspect_labels'][idx])\n","    start = np.where(x1 == 'B')[0][0]\n","    if not len(np.where(x1 == 'I')[0]):\n","      end = start+1\n","    else:\n","      end = np.where(x1 == 'I')[0][-1]\n","      end = end+1\n","    # print(x1)\n","    # print(\"-----\")\n","    # print(start,end)\n","\n","    x = df['terms'][idx]\n","    text_left = list(x[:start])\n","    text_aspect = list(x[start:end])\n","    text_right = list(x[end:])\n","    tag_left = [len(text_left)-i for i in range(len(text_left))]\n","    tag_aspect = [0 for i in range(len(text_aspect))]\n","    tag_right = [i+1 for i in range(len(text_right))]\n","    position_tag = tag_left + tag_aspect + tag_right\n","    if len(position_tag) == 0:\n","        position_tag = [0]\n","\n","    times = max_length - len(position_tag)\n","    r = [0]*times\n","    position_tag = position_tag+r\n","    position_data[idx] = np.array(position_tag)\n","\n","  return text_data , aspect_data, position_data, sentiment \n","      \n","def prepare_text_file_test(df,max_length,v):\n","  # max_length = 80\n","\n","  my_df = getResults(df) \n","\n","  text_data , aspect_data, position_data, sentiment  = prepare_dataset(v, my_df, max_length)\n","\n","  return text_data , aspect_data, position_data, sentiment\n"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bPAcFoUecjZ3"},"source":["#### Creating Dataloader "]},{"cell_type":"code","metadata":{"id":"xD3x82APclRM","executionInfo":{"status":"ok","timestamp":1638095277050,"user_tz":-330,"elapsed":8,"user":{"displayName":"Akanksha Shrimal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01375761960509741258"}}},"source":["class SentenceDataset(Dataset):\n","    \n","    def __init__(self, text_data , aspect_data, position_data, sentiment):\n","        data = list()\n","       \n","        for idx , text in enumerate(text_data):\n","          aspect_term = aspect_data[idx]\n","          position = position_data[idx]\n","          polarity = sentiment[idx]\n","          data.append({'text': torch.tensor(text,dtype=int), 'aspect': torch.tensor(aspect_term,dtype=int), 'position': torch.tensor(position,dtype=int), 'polarity':torch.tensor(polarity,dtype=int) })\n","\n","        self._data = data\n","    \n","    def __getitem__(self, index):\n","        return self._data[index]\n","    \n","    def __len__(self):\n","        return len(self._data)\n"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kD0cQCcAeQIu"},"source":["#### Parameters used "]},{"cell_type":"code","metadata":{"id":"BsL8gtDdbpBp","executionInfo":{"status":"ok","timestamp":1638095277051,"user_tz":-330,"elapsed":8,"user":{"displayName":"Akanksha Shrimal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01375761960509741258"}}},"source":["\n","params = { \n","'lstm': {\n","'data' :  ['text'],\n","'hyperparameters':  {'max_length' : 80,\n","'position_dim' : 100,\n","'embed_dim' : 300,\n","'hidden_dim' : 5,\n","'polarities_dim' : 3,\n","'epochs': 20 , \n","'initializer': torch.nn.init.xavier_uniform_ , \n","'optimizer':torch.optim.Adam, \n","'lr': 1e-4 , \n","\n","  }  , \n","\n","},\n","'aelstm' : {\n","        'data' :  ['text', 'aspect'],\n","'hyperparameters': {'max_length' : 80,\n","'position_dim' : 100,\n","'embed_dim' : 300,\n","'hidden_dim' : 200,\n","'polarities_dim' : 3,\n","'epochs': 20 , \n","'initializer': torch.nn.init.xavier_uniform_ , \n","'optimizer': torch.optim.Adam, \n","'lr': 1e-3 , \n","\n","  }  , \n","\n","},\n","'pban' : {\n","        'data' : ['text', 'aspect', 'position'] ,\n","'hyperparameters': {'max_length' : 80,\n","'position_dim' : 100,\n","'embed_dim' : 300,\n","'hidden_dim' : 200,\n","'polarities_dim' : 3,\n","  'epochs': 20 , \n","'initializer': torch.nn.init.xavier_uniform_ , \n","'optimizer':torch.optim.Adam, \n","'lr': 1e-3 , \n","\n","  }  \n","}\n","}\n"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"SXOGFnZjbpI7","executionInfo":{"status":"ok","timestamp":1638095277051,"user_tz":-330,"elapsed":8,"user":{"displayName":"Akanksha Shrimal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01375761960509741258"}}},"source":[""],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vm-S3KBw6zp7"},"source":["# Parameters"]},{"cell_type":"code","metadata":{"id":"F5454QFm4peL","executionInfo":{"status":"ok","timestamp":1638095277051,"user_tz":-330,"elapsed":7,"user":{"displayName":"Akanksha Shrimal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01375761960509741258"}}},"source":["# #baseline\n","# #Parameters\n","# model_type = 'lstm'         # 'lstm','cnn'\n","# embedding_type = 'double'  #  'triple' ,'double','bert'\n","# dataset_name = 'laptop'         # 'laptop', 'restaraunt'\n","\n","# #Label Indices\n","# tag_indices = {'B':0, 'I':1, 'O':2}\n","# rev_tag_indices = {0:'B', 1:'I', 2:'O'}"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"hOV38BD5Mwb_","executionInfo":{"status":"ok","timestamp":1638095651195,"user_tz":-330,"elapsed":574,"user":{"displayName":"Akanksha Shrimal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01375761960509741258"}}},"source":["#Parameters\n","model_type = 'cnn'         # 'lstm','cnn'\n","embedding_type = 'triple'  #  'triple' ,'double','bert'\n","dataset_name = 'restaraunt'         # 'laptop', 'restaraunt'\n","\n","#Label Indices\n","tag_indices = {'B':0, 'I':1, 'O':2}\n","rev_tag_indices = {0:'B', 1:'I', 2:'O'}"],"execution_count":27,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"seKsd32j62EF"},"source":["Load Dataset"]},{"cell_type":"code","metadata":{"id":"ZRl5Lmr_4wOc","executionInfo":{"status":"ok","timestamp":1638095655168,"user_tz":-330,"elapsed":1088,"user":{"displayName":"Akanksha Shrimal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01375761960509741258"}}},"source":["p = \"\"\n","laptop_train_set = load('/content/drive/MyDrive/NLP/Akanksha_files/data_pkl_files_laptop/train.pkl')\n","laptop_test_set = load('/content/drive/MyDrive/NLP/Akanksha_files/data_pkl_files_laptop/test.pkl')\n","\n","restaraunt_train_set = load('/content/drive/MyDrive/NLP/Akanksha_files/data_pkl_files_restraunt/train.pkl')\n","restaraunt_test_set = load('/content/drive/MyDrive/NLP/Akanksha_files/data_pkl_files_restraunt/test.pkl')\n","\n","if dataset_name == 'restaraunt':\n","  current_train_dataset = restaraunt_train_set\n","  current_test_dataset = restaraunt_test_set\n","else:\n","  current_train_dataset = laptop_train_set\n","  current_test_dataset = laptop_test_set\n","\n","max_len = 0\n","for sample in current_train_dataset:\n","  if len(sample['terms']) > max_len:\n","    max_len = len(sample['terms'])"],"execution_count":28,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iDdlURsu64DO"},"source":["Load Train Test Embedding"]},{"cell_type":"code","metadata":{"id":"APwgF1b95SIK","executionInfo":{"status":"ok","timestamp":1638095686415,"user_tz":-330,"elapsed":31250,"user":{"displayName":"Akanksha Shrimal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01375761960509741258"}}},"source":["p = \"/content/drive/MyDrive/NLP\"\n","X_train = load(p + '/Final/Embeddings/X_train_'+ embedding_type+'_'+dataset_name+'.pkl')\n","X_test = load(p + '/Final/Embeddings/X_test_'+ embedding_type +'_'+dataset_name+'.pkl')\n","y_train = load(p + '/Final/Embeddings/y_train_' + dataset_name +'.pkl')\n","y_test = load(p + '/Final/Embeddings/y_test_' + dataset_name +'.pkl')"],"execution_count":29,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6wrKkV-l-fG0"},"source":["Load Model"]},{"cell_type":"code","metadata":{"id":"BqAy-bly-Krz","executionInfo":{"status":"ok","timestamp":1638095687534,"user_tz":-330,"elapsed":1122,"user":{"displayName":"Akanksha Shrimal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01375761960509741258"}}},"source":["model = keras.models.load_model(p + '/Final/Models/ATE/'+dataset_name+'_'+embedding_type+'_'+model_type+'.pkl')"],"execution_count":30,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XBMwm3yX6_o6"},"source":["Predictions"]},{"cell_type":"code","metadata":{"id":"J3FjVx7x5wop","executionInfo":{"status":"ok","timestamp":1638095692789,"user_tz":-330,"elapsed":5259,"user":{"displayName":"Akanksha Shrimal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01375761960509741258"}}},"source":["train_pred = np.argmax(model.predict(X_train),axis = 2)\n","train_target = np.argmax(np.array(y_train),axis=2)\n","test_pred = np.argmax(model.predict(X_test),axis=2)\n","test_target = np.argmax(np.array(y_test),axis=2)\n","\n","test_y_t = []\n","test_y_h = []\n","for i in range(test_pred.shape[0]):\n","  for j in range(test_pred.shape[1]):\n","    test_y_t.append(test_target[i,j])\n","    test_y_h.append(test_pred[i,j])\n","train_y_t = []\n","train_y_h = []\n","for i in range(train_pred.shape[0]):\n","  for j in range(train_pred.shape[1]):\n","    train_y_t.append(train_target[i,j])\n","    train_y_h.append(train_pred[i,j])"],"execution_count":31,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8HXAVdHc7CdY"},"source":["Evaluation"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8vvhiSVR6GBz","executionInfo":{"status":"ok","timestamp":1638095693740,"user_tz":-330,"elapsed":966,"user":{"displayName":"Akanksha Shrimal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01375761960509741258"}},"outputId":"f2ad8e67-12e3-4803-f4c7-521833f7b8a9"},"source":["from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import classification_report\n","def tag_wise_metrics(y_true, y_pred, target_names):\n","  print(classification_report(y_true, y_pred, target_names=target_names))\n","print(\"=================================Train Dataset=================================\")\n","tag_wise_metrics(train_y_t, train_y_h, ['B','I','O'])\n","print(\"=================================Test Dataset=================================\")\n","tag_wise_metrics(test_y_t, test_y_h, ['B','I','O'])\n"],"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["=================================Train Dataset=================================\n","              precision    recall  f1-score   support\n","\n","           B       0.98      0.97      0.98      3269\n","           I       0.98      0.95      0.97      1231\n","           O       1.00      1.00      1.00    200700\n","\n","    accuracy                           1.00    205200\n","   macro avg       0.99      0.98      0.98    205200\n","weighted avg       1.00      1.00      1.00    205200\n","\n","=================================Test Dataset=================================\n","              precision    recall  f1-score   support\n","\n","           B       0.84      0.73      0.78      1121\n","           I       0.84      0.57      0.68       502\n","           O       0.99      1.00      1.00     58377\n","\n","    accuracy                           0.99     60000\n","   macro avg       0.89      0.77      0.82     60000\n","weighted avg       0.99      0.99      0.99     60000\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"8PcNezR87ESW"},"source":["Output to Forward"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"lEoHGn-k7U6Y","executionInfo":{"status":"ok","timestamp":1638095693740,"user_tz":-330,"elapsed":25,"user":{"displayName":"Akanksha Shrimal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01375761960509741258"}},"outputId":"ab3b171c-6419-4f10-c113-d7583aa84711"},"source":["terms = [[t for t in sample['terms']] for sample in current_train_dataset]\n","pred_labels = [list(train_pred[i])[:len(terms[i])] for i in range(train_pred.shape[0])]\n","pred_labels = [[rev_tag_indices[t] for t in sample] for sample in pred_labels]\n","sentiments = [sample['sentiment'] for sample in current_train_dataset]\n","train_final = [[terms[i],pred_labels[i],sentiments[i]] for i in range(len(terms))]\n","train_final = pd.DataFrame(train_final,columns=[\"terms\",\"aspect_labels\",\"sentiments\"])\n","train_final.head()"],"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>terms</th>\n","      <th>aspect_labels</th>\n","      <th>sentiments</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[but, the, staff, was, so, horrible, to, us, .]</td>\n","      <td>[O, O, B, O, O, O, O, O, O]</td>\n","      <td>[NA, NA, NEG, NA, NA, NA, NA, NA, NA]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[to, be, completely, fair, ,, the, only, redee...</td>\n","      <td>[O, O, O, O, O, O, O, O, O, O, O, B, O, O, O, ...</td>\n","      <td>[NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, P...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[the, food, is, uniformly, exceptional, ,, wit...</td>\n","      <td>[O, B, O, O, O, O, O, O, O, O, B, O, O, O, O, ...</td>\n","      <td>[NA, POS, NA, NA, NA, NA, NA, NA, NA, NA, POS,...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>[where, gabriela, personaly, greets, you, and,...</td>\n","      <td>[O, O, O, O, O, O, O, O, O, O, O, O]</td>\n","      <td>[NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>[not, only, was, the, food, outstanding, ,, bu...</td>\n","      <td>[O, O, O, O, B, O, O, O, O, O, B, O, O, O]</td>\n","      <td>[NA, NA, NA, NA, POS, NA, NA, NA, NA, NA, POS,...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                               terms  ...                                         sentiments\n","0    [but, the, staff, was, so, horrible, to, us, .]  ...              [NA, NA, NEG, NA, NA, NA, NA, NA, NA]\n","1  [to, be, completely, fair, ,, the, only, redee...  ...  [NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, P...\n","2  [the, food, is, uniformly, exceptional, ,, wit...  ...  [NA, POS, NA, NA, NA, NA, NA, NA, NA, NA, POS,...\n","3  [where, gabriela, personaly, greets, you, and,...  ...   [NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA]\n","4  [not, only, was, the, food, outstanding, ,, bu...  ...  [NA, NA, NA, NA, POS, NA, NA, NA, NA, NA, POS,...\n","\n","[5 rows x 3 columns]"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"EDc35nFp-tf1","executionInfo":{"status":"ok","timestamp":1638095693741,"user_tz":-330,"elapsed":23,"user":{"displayName":"Akanksha Shrimal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01375761960509741258"}},"outputId":"ecd8bd34-383d-473c-9833-eb959873cf84"},"source":["terms = [[t for t in sample['terms']] for sample in current_test_dataset]\n","pred_labels = [list(test_pred[i])[:len(terms[i])] for i in range(test_pred.shape[0])]\n","pred_labels = [[rev_tag_indices[t] for t in sample] for sample in pred_labels]\n","sentiments = [sample['sentiment'] for sample in current_test_dataset]\n","train_final2 = [[terms[i],pred_labels[i],sentiments[i]] for i in range(len(terms))]\n","train_final2 = pd.DataFrame(train_final2,columns=[\"terms\",\"aspect_labels\",\"sentiments\"])\n","train_final2.head()"],"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>terms</th>\n","      <th>aspect_labels</th>\n","      <th>sentiments</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[the, bread, is, top, notch, as, well, .]</td>\n","      <td>[O, B, O, O, O, O, O, O]</td>\n","      <td>[NA, POS, NA, NA, NA, NA, NA, NA]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[i, have, to, say, they, have, one, of, the, f...</td>\n","      <td>[O, O, O, O, O, O, O, O, O, O, B, O, O, O, O, O]</td>\n","      <td>[NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, POS, ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[food, is, always, fresh, and, hot, ready, to,...</td>\n","      <td>[B, O, O, O, O, O, O, O, O, O]</td>\n","      <td>[POS, NA, NA, NA, NA, NA, NA, NA, NA, NA]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>[did, i, mention, that, the, coffee, is, outst...</td>\n","      <td>[O, O, O, O, O, B, O, O, O]</td>\n","      <td>[NA, NA, NA, NA, NA, POS, NA, NA, NA]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>[certainly, not, the, best, sushi, in, new, yo...</td>\n","      <td>[O, O, O, O, B, O, O, O, O, O, O, O, O, O, O, ...</td>\n","      <td>[NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                               terms  ...                                         sentiments\n","0          [the, bread, is, top, notch, as, well, .]  ...                  [NA, POS, NA, NA, NA, NA, NA, NA]\n","1  [i, have, to, say, they, have, one, of, the, f...  ...  [NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, POS, ...\n","2  [food, is, always, fresh, and, hot, ready, to,...  ...          [POS, NA, NA, NA, NA, NA, NA, NA, NA, NA]\n","3  [did, i, mention, that, the, coffee, is, outst...  ...              [NA, NA, NA, NA, NA, POS, NA, NA, NA]\n","4  [certainly, not, the, best, sushi, in, new, yo...  ...  [NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N...\n","\n","[5 rows x 3 columns]"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","metadata":{"id":"Qdt9LGVt-wu-","executionInfo":{"status":"ok","timestamp":1638095693741,"user_tz":-330,"elapsed":22,"user":{"displayName":"Akanksha Shrimal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01375761960509741258"}}},"source":["# train_final['aspect_labels'][0]"],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"id":"IX5z6w3P7jcZ","executionInfo":{"status":"ok","timestamp":1638095693742,"user_tz":-330,"elapsed":23,"user":{"displayName":"Akanksha Shrimal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01375761960509741258"}}},"source":["# train_final2"],"execution_count":36,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2J_pXwAYfCc_"},"source":["# ATC"]},{"cell_type":"markdown","metadata":{"id":"tSMLwT0BfE33"},"source":["#### Telling model and hyper param used later on "]},{"cell_type":"code","metadata":{"id":"gu7j192cfKxH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638095884882,"user_tz":-330,"elapsed":191162,"user":{"displayName":"Akanksha Shrimal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01375761960509741258"}},"outputId":"a8e9047f-eb1c-4500-a52d-f06d7c2c4464"},"source":["max_length = 80\n","\n","x = input(\"Name the model you want to input 'lstm, aelstm, pban : \")\n","model_name = x\n","model_data = params[model_name]['data'] \n","model_hyper_param = params[model_name]['hyperparameters'] \n","\n","x = input(\"Enter dataset to be used 1. Restraunt 2.Laptop \")\n","base = '/content/drive/MyDrive/NLP'\n","if int(x) ==1:\n","  vocab = load( base + '/Akanksha_files/data_pkl_files_restraunt/vocab.pkl')\n","  embedding_matrix = load( base + '/Akanksha_files/data_pkl_files_restraunt/embedding_matrix.pkl')\n","\n","else:\n","\n","  vocab = load(base + '/Akanksha_files/data_pkl_files_laptop/vocab.pkl')\n","  embedding_matrix = load(base + '/Akanksha_files/data_pkl_files_laptop/embedding_matrix.pkl')"],"execution_count":37,"outputs":[{"name":"stdout","output_type":"stream","text":["Name the model you want to input 'lstm, aelstm, pban : pban\n","Enter dataset to be used 1. Restraunt 2.Laptop 1\n"]}]},{"cell_type":"markdown","metadata":{"id":"PYsfYxU2fLbe"},"source":["#### Processing further data"]},{"cell_type":"code","metadata":{"id":"3YJgtAmgfNq6","executionInfo":{"status":"ok","timestamp":1638095425805,"user_tz":-330,"elapsed":8157,"user":{"displayName":"Akanksha Shrimal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01375761960509741258"}}},"source":["# training \n","train_text_data , train_aspect_data, train_position_data, train_sentiment = prepare_text_file_test(train_final,max_length,vocab)"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"jba95lcffDva","executionInfo":{"status":"ok","timestamp":1638095427949,"user_tz":-330,"elapsed":2147,"user":{"displayName":"Akanksha Shrimal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01375761960509741258"}}},"source":["# testing \n","test_text_data , test_aspect_data, test_position_data, test_sentiment = prepare_text_file_test(train_final2,max_length,vocab)"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"NGiZ9t5agIwG","executionInfo":{"status":"ok","timestamp":1638095429362,"user_tz":-330,"elapsed":661,"user":{"displayName":"Akanksha Shrimal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01375761960509741258"}}},"source":["# create data loaders \n","trainset = SentenceDataset(train_text_data , train_aspect_data, train_position_data, train_sentiment)\n","testset = SentenceDataset( test_text_data , test_aspect_data, test_position_data, test_sentiment)\n","\n","train_dataloader = DataLoader(dataset=trainset, batch_size=64, shuffle=True)\n","test_dataloader = DataLoader(dataset=testset, batch_size=64, shuffle=False)"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"6aaBb44XgXfX","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1f73ddd1-85ec-48a5-9310-07df4ed1e87b"},"source":["model_name , model_data , model_hyper_param "],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('pban',\n"," ['text', 'aspect', 'position'],\n"," {'embed_dim': 300,\n","  'epochs': 20,\n","  'hidden_dim': 200,\n","  'initializer': <function torch.nn.init.xavier_uniform_>,\n","  'lr': 0.001,\n","  'max_length': 80,\n","  'optimizer': torch.optim.adam.Adam,\n","  'polarities_dim': 3,\n","  'position_dim': 100})"]},"metadata":{},"execution_count":130}]},{"cell_type":"code","metadata":{"id":"65FlbrxHgX-3"},"source":["\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n","if model_name == 'lstm':\n","  model = LSTM(embedding_matrix,model_hyper_param).to(device)\n","elif model_name == 'aelstm':\n","  model = AE_LSTM(embedding_matrix,model_hyper_param).to(device)\n","else:\n","  model = PBAN(embedding_matrix,model_hyper_param).to(device)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-HHaN7-XgaHx"},"source":["criterion = nn.CrossEntropyLoss()\n","_params = filter(lambda p: p.requires_grad, model.parameters())\n","optimizer = torch.optim.Adam(_params, lr=model_hyper_param['lr'], weight_decay=1e-5)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GML2DBGZggmA"},"source":["def replicate_results(path, model_name,test_dataloader,model_hyper_param):\n","# path of saved model \n","  \n","  # model \n","  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n","  if model_name == 'lstm':\n","    model = LSTM(embedding_matrix,model_hyper_param).to(device)\n","  elif model_name == 'aelstm':\n","    model = AE_LSTM(embedding_matrix,model_hyper_param).to(device)\n","  else:\n","    model = PBAN(embedding_matrix,model_hyper_param).to(device)\n","\n","  # load and evaluate \n","  model.load_state_dict(torch.load(path))\n","  model.eval()\n","  test_acc, f1 = evaluate(model,test_dataloader)\n","  print(\"Accuracy , F1 score\")\n","  print(test_acc, f1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XCcTAsgigjCJ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b962132f-1c61-451a-d0d6-652405add659"},"source":["model_name = input(\"Enter the model name : lstm, aelstm, pban : \")\n","x = input(\"Enter dataset : 1.Restraunt 2.Laptop\")\n","\n","# best_path with respect to each model and dataset\n","# 1 is rest , 2 is laptop \n","if model_name == 'lstm' and int(x) == 1:\n","  best_path = base + '/Akanksha_files/data_pkl_files_restraunt/models/lstm/state_dict/lstm_res_3f10.3224_19'\n","elif model_name == 'aelstm' and int(x) == 1:\n","  best_path = base + '/Akanksha_files/data_pkl_files_restraunt/models/aelstm/state_dict/lstm_res_3f10.5655_12'\n","elif model_name == 'pban' and int(x) == 1:\n","  best_path = base + '/Akanksha_files/data_pkl_files_restraunt/models/pban/state_dict/lstm_res_3f10.6684_17'\n","elif model_name == 'lstm' and int(x) == 2:\n","  best_path = base + '/Akanksha_files/data_pkl_files_laptop/models/lstm/state_dict/lstm_res_3f10.3946_19'\n","elif model_name == 'aelstm' and int(x) == 2:\n","  best_path = base + '/Akanksha_files/data_pkl_files_laptop/models/aelstm/state_dict/lstm_res_3f10.5260_10'\n","elif model_name == 'pban' and int(x) == 2:\n","  best_path = base + '/Akanksha_files/data_pkl_files_laptop/models/pban/state_dict/lstm_res_3f10.6111_9'\n","\n","path = best_path \n","print(\"Testing\")\n","replicate_results(path, model_name,test_dataloader,model_hyper_param)\n","print(\"Training\")\n","replicate_results(path, model_name,train_dataloader,model_hyper_param)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Enter the model name : lstm, aelstm, pban : pban\n","Enter dataset : 1.Restraunt 2.Laptop1\n","Testing\n","Accuracy , F1 score\n","0.7267144319344934 0.6247275059590686\n","Training\n","Accuracy , F1 score\n","0.7838421214924453 0.7044096839861195\n"]}]},{"cell_type":"code","metadata":{"id":"nSh8g0pwDLNK"},"source":["# embediing triple model type cnn "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rMsIdgmUNK7b"},"source":["# BASELINE \n","# LSTM DOUBLE \n","# LSTM CLASSIFICATION \n","\n","# restraunt \n","# Enter the model name : lstm, aelstm, pban : lstm\n","# Enter dataset : 1.Restraunt 2.Laptop1\n","# Testing\n","# Accuracy , F1 score\n","# 0.6136125654450262 0.3129938405011427\n","# Training\n","# Accuracy , F1 score\n","# 0.5713324360699865 0.3534244748170867\n","\n","# Laptop \n","# Enter the model name : lstm, aelstm, pban : lstm\n","# Enter dataset : 1.Restraunt 2.Laptop2\n","# Testing\n","# Accuracy , F1 score\n","# 0.5515463917525774 0.4267538090911967\n","# Training\n","# Accuracy , F1 score\n","# 0.436498150431566 0.37061853059486594"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YxSS3HzYNkUb"},"source":["# Final Model \n","# CNN Triple \n","# PBAN \n","\n","# restaurant \n","# Enter the model name : lstm, aelstm, pban : pban\n","# Enter dataset : 1.Restraunt 2.Laptop1\n","# Testing\n","# Accuracy , F1 score\n","# 0.7267144319344934 0.6247275059590686\n","# Training\n","# Accuracy , F1 score\n","# 0.7838421214924453 0.7044096839861195\n","\n","\n","\n","# # Laptop \n","# Enter the model name : lstm, aelstm, pban : pban\n","# Enter dataset : 1.Restraunt 2.Laptop2\n","# Testing\n","# Accuracy , F1 score\n","# 0.6019736842105263 0.5543710516536603\n","# Training\n","# Accuracy , F1 score\n","# 0.82392667631452 0.8071314322140791\n"],"execution_count":null,"outputs":[]}]}